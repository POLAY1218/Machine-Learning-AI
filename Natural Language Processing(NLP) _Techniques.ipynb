{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee30fc69",
   "metadata": {},
   "source": [
    "## Natural Language Processing ::\n",
    "\n",
    "### 1.Tokenization\n",
    "### 2.Stemming\n",
    "### 3.Lemmatization\n",
    "### 4.Stop words\n",
    "### 5.POS Tagging\n",
    "### 6.Chunking\n",
    "### 7.Name Entitiy Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69474303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "# btw ntlk.tag is the library and pos_tag is the class(similar w the rest abv and blw)\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7453ba26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405d3ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing datasets\n",
    "dataset=\"Hello Everyone. Wecome to this course. We are studying NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d9ea997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Everyone.', 'Wecome to this course.', 'We are studying NLP']\n",
      "Hello Everyone.\n",
      "Wecome to this course.\n",
      "We are studying NLP\n",
      "['Hello', 'Everyone', '.', 'Wecome', 'to', 'this', 'course', '.', 'We', 'are', 'studying', 'NLP']\n",
      "Hello\n",
      "Everyone\n",
      ".\n",
      "Wecome\n",
      "to\n",
      "this\n",
      "course\n",
      ".\n",
      "We\n",
      "are\n",
      "studying\n",
      "NLP\n"
     ]
    }
   ],
   "source": [
    "# tokenizing sentences first\n",
    "print(sent_tokenize(\n",
    "    text=dataset, language=\"english\"))\n",
    "for i in sent_tokenize(text=dataset, language=\"english\"):\n",
    "    print(i) #printing each sentences separately\n",
    "# tokenizing words\n",
    "print(word_tokenize(text=dataset, language=\"english\"))\n",
    "for i in word_tokenize(text=dataset, language=\"english\"):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227a4ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\n",
      "love\n",
      "lover\n",
      "love\n",
      "lovingli\n",
      "it\n",
      "feel\n",
      "veri\n",
      "speical\n",
      "when\n",
      "you\n",
      "are\n",
      "love\n",
      "someon\n",
      ".\n",
      "we\n",
      "care\n",
      "for\n",
      "our\n",
      "love\n",
      "one\n",
      ".\n",
      "special\n",
      "when\n",
      "we\n",
      "love\n",
      "each\n",
      "other\n",
      "uncodit\n"
     ]
    }
   ],
   "source": [
    "# stemming::\n",
    "dataset = ['love', 'loving', 'lover', 'loved', 'lovingly']\n",
    "# apply stemming\n",
    "ps = PorterStemmer()\n",
    "for i in dataset:\n",
    "    print(ps.stem(i))\n",
    "#before stemming we obviously have to tokenize ; next is an example where both steps are being executed sequentially\n",
    "new_dataset = \"It feels very speical when you are loving someone. We care for our loved ones. Specially when we love each other uncoditionally\"\n",
    "words = word_tokenize(\n",
    "    new_dataset)  # before stemming we have to tokenize the words\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5caa8b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmatization::\n",
    "\n",
    "# apply limmatization\n",
    "wnl = WordNetLemmatizer()\n",
    "wnl.lemmatize('churches')\n",
    "\n",
    "wnl.lemmatize('doggy')\n",
    "\n",
    "wnl.lemmatize('feet')\n",
    "wnl.lemmatize('better')\n",
    "# pos='a' pos means part of speech,  we want our part of speech ie our word to be consdiered as adjective then we have to pass  parameter ,\n",
    "# for that , otherwise system would automatically consider it as noun\n",
    "\n",
    "wnl.lemmatize('better', pos='a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a0d2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116f3606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "['Hello', 'Mr.Waston', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'awesome', '.', 'The', 'garden', 'is', 'Green', '.', 'We', 'should', 'go', 'out', 'for', 'a', 'walk']\n",
      "['Hello', 'Mr.Waston', ',', 'today', '?', 'The', 'weather', 'awesome', '.', 'The', 'garden', 'Green', '.', 'We', 'go', 'walk']\n"
     ]
    }
   ],
   "source": [
    "# stop words::\n",
    "# stop words\n",
    "dataset = 'Hello Mr.Waston, how are you doing today? The weather is awesome. The garden is Green. We should go out for a walk'\n",
    "# create a set of english stop words\n",
    "stop_words = (stopwords.words('english'))\n",
    "print(stop_words)\n",
    "# after performing stop words we have to tokenize the words\n",
    "wordsTokenize = word_tokenize(dataset)\n",
    "print(wordsTokenize)\n",
    "# remove stop word from empty dataset\n",
    "filtered_sentences = []\n",
    "\n",
    "for w in wordsTokenize:\n",
    "    if w not in stop_words:\n",
    "        # now the output would have all the stop words eliminated\n",
    "        filtered_sentences.append(w)\n",
    "print(filtered_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac82e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# POS tagging::\n",
    "dataset = \"Taj Mahal is one of the world's most celebrated structures. It is a stunnging symbol of Indian history\"\n",
    "# Before POS tagging we have to word tokenize\n",
    "wordsTokenize = word_tokenize(dataset)\n",
    "# apply pos tagging\n",
    "# before we understand some of the tags NN,DT,VBZ,POSCD,JJ etc we see in the output we have to understand tag set\n",
    "pos_tag(wordsTokenize)\n",
    "# tag set\n",
    "nltk.help.upenn_tagset()  # this would show us what each tagset means including examples for clarification.\n",
    "# JJR is comparative adejective like calmer, dearer etc while JJS is superlative adjective like calmest, dearest etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9b5217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14eda986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (chunk Taj/NNP Mahal/NNP)\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (chunk world/NN)\n",
      "  's/POS\n",
      "  most/RBS\n",
      "  celebrated/JJ\n",
      "  structures/NNS\n",
      "  ./.\n",
      "  It/PRP\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  stunnging/JJ\n",
      "  (chunk symbol/NN)\n",
      "  of/IN\n",
      "  Indian/JJ\n",
      "  (chunk history/NN))\n"
     ]
    }
   ],
   "source": [
    "# chunking:: we have to word tokenize and pos tagging before chunking, since pos_tag(wordsTokenize) abv has both\n",
    "#those steps covered, we would use that piece of code to do chunking\n",
    "dataset = \"Taj Mahal is one of the world's most celebrated structures. It is a stunnging symbol of Indian history\"\n",
    "# Before POS tagging we have to word tokenize\n",
    "wordsTokenize = word_tokenize(dataset)\n",
    "# apply pos tagging\n",
    "# before we understand some of the tags NN,DT,VBZ,POSCD,JJ etc we see in the output we have to understand tag set\n",
    "postagging = pos_tag(wordsTokenize)\n",
    "postagging\n",
    "# now we have to define sequence of chunks\n",
    "sequence_chunk = \"\"\"\n",
    "chunk:\n",
    "    {<NNPS>+}\n",
    "    {<NNP>+}\n",
    "    {<NN>+}\n",
    "\"\"\"  # btw \"\"\"   \"\"\" is part of the chunk sequence syntax\n",
    "# see jupyter notebook file for explanation of the code abv\n",
    "\n",
    "# create object w regular expression\n",
    "chunk = RegexpParser(sequence_chunk)\n",
    "\n",
    "# apply chunking\n",
    "chunk_result = chunk.parse(postagging)\n",
    "print(chunk_result)  # the output shows only Taj Mahal(chunk Taj/NNP Mahal/NNP) those two words are\n",
    "# chunked together.no other chunking of words are present in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5164f836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a86a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "dataset = \"Abraham Lincoln was an American stateman and lawyer who served as the 16th President of the United States\"\n",
    "# first we have apply tokenization and pos tagging(below we are doing both at the same time using chain functions)\n",
    "dataset_tag = pos_tag(word_tokenize(dataset))\n",
    "# apply named entity recognition\n",
    "dataset_ner = ne_chunk(dataset_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab590f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89830c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Abraham/NNP)\n",
      "  (PERSON Lincoln/NNP)\n",
      "  was/VBD\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  stateman/NN\n",
      "  and/CC\n",
      "  lawyer/NN\n",
      "  who/WP\n",
      "  served/VBD\n",
      "  as/IN\n",
      "  the/DT\n",
      "  16th/CD\n",
      "  President/NNP\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (GPE United/NNP States/NNPS))\n"
     ]
    }
   ],
   "source": [
    "# in the output in GPE American or GPE United , GPE means geoplotical entity\n",
    "print(dataset_ner)\n",
    "# we can also draw a tree diagram\n",
    "dataset_ner.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebf8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8b3e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
